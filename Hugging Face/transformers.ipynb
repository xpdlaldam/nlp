{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvXu2EAydTeD4Xds+1BuvL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xpdlaldam/nlp/blob/master/Hugging%20Face/transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. pipeline**"
      ],
      "metadata": {
        "id": "9BDZvqWu2kaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets gradio evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "zqrXOZf5ymnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "7d8HCNA16N41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "leNQSDmO6Qpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-1. sentiment analysis"
      ],
      "metadata": {
        "id": "MXY7YeXb269z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### sentiment-analysis\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "sents = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "    \"neutral i'd say\"\n",
        "    ]\n",
        "# classifier(sents[2]) # one by one\n",
        "classifier(sents) # simultaneous"
      ],
      "metadata": {
        "id": "KCVeM54tz2ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-2. customize labels"
      ],
      "metadata": {
        "id": "5INhzNrD3L0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### zero-shot-classification: lets customize labels\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "sents = [\n",
        "    \"this is biology 101\",\n",
        "    \"president trump\",\n",
        "    \"capex was over 1B this time\",\n",
        "]\n",
        "\n",
        "classifier(\n",
        "    sents,\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ],
      "metadata": {
        "id": "IgzupHno1R2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-3. generate text"
      ],
      "metadata": {
        "id": "UPN47d4k3U1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# distilgpt2\n",
        "# deepset/roberta-base-squad2\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\"summarize AMD's most recent financial report\")"
      ],
      "metadata": {
        "id": "AfHJAQ_U3dq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(model=\"FacebookAI/roberta-large-mnli\")\n",
        "\n",
        "sents = [\n",
        "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
        "    \"I hate this so much!\",\n",
        "    \"neutral i'd say\"\n",
        "    ]\n",
        "\n",
        "pipe(sents)"
      ],
      "metadata": {
        "id": "MUqlBsL65uuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-4. speech recognition"
      ],
      "metadata": {
        "id": "IJFuQf20AWVp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\", device=0)\n",
        "dataset = load_dataset(\"superb\", name=\"asr\", split=\"test\")\n",
        "\n",
        "# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\n",
        "# as we're not interested in the *target* part of the dataset. For sentence pair use KeyPairDataset\n",
        "for out in tqdm(pipe(KeyDataset(dataset, \"file\"))):\n",
        "    print(out)\n",
        "    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n",
        "    # {\"text\": ....}\n",
        "    # ...."
      ],
      "metadata": {
        "id": "GF7f2b1G7N-X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "UiadjDkn9seH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import list_datasets\n",
        "print([dataset.id for dataset in list_datasets()])"
      ],
      "metadata": {
        "id": "ibLa_wrs9g0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds = load_dataset(\"PolyAI/minds14\", name=\"ko-KR\", split=\"train\")\n",
        "minds"
      ],
      "metadata": {
        "id": "Nl6CJJ4-AQeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds[0]"
      ],
      "metadata": {
        "id": "rqaP61IUB10N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = minds.features[\"intent_class\"].int2str\n",
        "id2label(minds[0][\"intent_class\"])"
      ],
      "metadata": {
        "id": "YKI-WibVC8Qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds.shuffle()[0]"
      ],
      "metadata": {
        "id": "yMudptalFJ38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def generate_audio():\n",
        "    example = minds[0]\n",
        "    audio = example[\"audio\"]\n",
        "    return (\n",
        "        audio[\"sampling_rate\"], # Hz\n",
        "        audio[\"array\"], # contains the sound represented in numbers in an array\n",
        "    ), id2label(example[\"intent_class\"])\n",
        "\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Column():\n",
        "        for _ in range(1):\n",
        "            audio, label = generate_audio()\n",
        "            output = gr.Audio(audio, label=label)\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "k-mS3ZBtCCdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example = minds[0]\n",
        "example['audio']"
      ],
      "metadata": {
        "id": "JKlerrDxR_hR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## visualize\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "\n",
        "example = minds[0]\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "librosa.display.waveshow(example[\"audio\"][\"array\"], sr=example[\"audio\"][\"sampling_rate\"])"
      ],
      "metadata": {
        "id": "_T71tBolRnD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1-5. Fill in the blank"
      ],
      "metadata": {
        "id": "o11j6qaGehjS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"The cheapest flight from EWR to ICN is <mask>.\", top_k=5)"
      ],
      "metadata": {
        "id": "mANyJcY2daDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"Elon Musk and Trump are on the same boat\")"
      ],
      "metadata": {
        "id": "0x6csmW_fBRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-tc-big-ko-en\")\n",
        "pipe(\"설악산은 한국의 100대 명산 중 하나이다\")"
      ],
      "metadata": {
        "id": "-23kubunkRJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. The Transformer architecture"
      ],
      "metadata": {
        "id": "PPzybyibXrvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The Transformer model is based on Attention is All you Need\n",
        "# Attention is All you Need is a encoder-decoder (aka sequence to sequence transformer) model where it has encoders and decoders\n",
        "# The encoder \"encodes\" text into numerical representations\n",
        "# These numerical representations are all also called \"embeddings\" or \"features\"\n",
        "# The decoder \"decodes\" the representations from the encoder"
      ],
      "metadata": {
        "id": "bEm9BwYXXvcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-1. Encoder models"
      ],
      "metadata": {
        "id": "_46W3vZmFOUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ex) Welcome to Korea => each word has its own numerical representation (aka feature vector or tensor vector) comprising of sequence of numbers\n",
        "# ex) \"Welcome\" corresponds to [.1, .3, .2, ...]\n",
        "\n",
        "# ex) BERT\n",
        "\n",
        "# The feature vector looks at the left and right of the word, hence capturing \"context\" of the sentence thanks to the self-attention mechanism\n",
        "\n",
        "# Encoder models are good at obtaining an understanding of sequences and the relationship b/w words\n",
        "  # Extracting meaningful information\n",
        "  # Masked Language Modeling: guessing a randomly masked work\n",
        "  # Classification => Sentiment analysis ex) is the sentence positive or negative"
      ],
      "metadata": {
        "id": "Sn7QPJh5FR93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2-2. Decoders"
      ],
      "metadata": {
        "id": "df80_4c-Z6d_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### great at:\n",
        "## uni-directional: access to either the left or right context\n",
        "\n",
        "## causal tasks: guesses the next word in a sequence (auto-regressive)\n",
        "# ex) my => name\n",
        "# my name => is\n",
        "# my name is => Peter\n",
        "\n",
        "## generating sequences\n",
        "\n",
        "# words can only see the words on their left side; the right side is hidden\n",
        "# => means unidirectional"
      ],
      "metadata": {
        "id": "QesEtD4CZ9LO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2-3. Encoder-Decoder"
      ],
      "metadata": {
        "id": "NZkTEChceAX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## aka sequence-to-sequence model\n",
        "# step 1) the encoder takes a sequence of words such as \"Welcome to Korea\"\n",
        "# step 2) the encoder outputs a numerical represenation for each word\n",
        "# step 3) the decoder takes the first word Welcome as an input\n",
        "# step 4) the decoder outputs the second word based on the first word (at this point we don't need to use the encoder)\n",
        "\n",
        "##\n",
        "# the encoder takes cafe of understanding the sequence\n",
        "# the decoder takes care of generating a sequence according to the understanding of the encoder, hence it \"decodes\"\n",
        "# the weights are not necessarily shared b/w an encoder and a decoder\n",
        "\n",
        "## good at:\n",
        "# summarizing text"
      ],
      "metadata": {
        "id": "pZ7VE9BoeEkI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. How does the pipeline function work?"
      ],
      "metadata": {
        "id": "iEu2eSK9f2Dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## there are 3 stages\n",
        "## 1. Tokenizer\n",
        "# converts raw text into input ids\n",
        "# ex) hi my name is => [101, 2342, 1212, 2357]\n",
        "# the AutoTokenizer class can load the tokenizer for any checkpoint (language model)\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "raw_inputs = [\n",
        "    \"i'm pretty hungry right now\",\n",
        "    \"i need food now\",\n",
        "]\n",
        "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "inputs\n",
        "\n",
        "# padding=True: to make the sentences the same lenth\n",
        "# truncation=True: ensure any sentence does not exceed the maximum the model can handle\n",
        "# return_tensors=\"pt\": pt means pytorch tensor\n",
        "# attention_mask: indicates where padding was applied so the model does not pay attention to it\n",
        "# outputs a dictionary with two keys\n",
        "# input_ids: one row per sentence => unique identifiers of the tokens for each sentence"
      ],
      "metadata": {
        "id": "kVV6JJMvf7Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2. Model\n",
        "# Based on the input ids from the tokenizer, we now move on to the Model\n",
        "# part which outputs logits\n",
        "# outputs \"hidden states\" aka features\n",
        "# \"hidden states\" are usually inputs to another part of the model, known as the \"head\"\n",
        "# for each model input (sentence in this case), we get a high-dimensional vector that represents\n",
        "# the contextual understanding of that input comprising of\n",
        "# batch size:  number of sequences (sentences in this case)\n",
        "# sequence length\n",
        "# hidden size: the vector dimension of each model input\n",
        "from transformers import AutoModel\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModel.from_pretrained(checkpoint)\n",
        "\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# [2, 9, 768]\n",
        "# 2: number of sequences\n",
        "# 9: sequence length\n",
        "# 768: vector dimension of each model input\n",
        "outputs.last_hidden_state.shape"
      ],
      "metadata": {
        "id": "J7u5ihIyju7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3. Postprocessing\n",
        "# For our example, we need a model with a sequence classification head\n",
        "#\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "outputs = model(**inputs)\n",
        "\n",
        "# you can see that the dimensionality is much lower as the model head\n",
        "# took the high-dimensional input and outputted a 2 by 2 vector\n",
        "# (one per label => positive and negative for each sequence)\n",
        "print(outputs.logits.shape)\n",
        "\n",
        "# note that these numbers are logits (the raw, unnormalized scores)\n",
        "# , not probabilities\n",
        "# they need to go through a SoftMax layer to convert to probabilities\n",
        "# Q. why does all Transformers models outputs logits?\n",
        "# A. because the loss function for training will generally fuse\n",
        "# the last activation function such as a SoftMax, with the actual loss\n",
        "# such as cross entropy\n",
        "print(outputs.logits)\n",
        "import torch\n",
        "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(predictions)"
      ],
      "metadata": {
        "id": "GDrSA8RqjwD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model labels\n",
        "model.config.id2label"
      ],
      "metadata": {
        "id": "N8nKkLbOjVj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How To Instantiate a Transformers model"
      ],
      "metadata": {
        "id": "zibX4q35wA_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "6LYZ8kldwJtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### How to upload weights\n",
        "## Method 1\n",
        "from transformers import AutoConfig\n",
        "AutoConfig.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "## Method 2\n",
        "from transformers import BertConfig, BertModel\n",
        "model = BertModel.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "niRxbrY8wG2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Config file: a blueprint that contains all the info to create the model architecture\n",
        "BertConfig.from_pretrained(\"bert-base-cased\")"
      ],
      "metadata": {
        "id": "VxPxbBOVx8tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## to use different layers ex) use 10 layers instead of 12\n",
        "# however, this will start with randomly assigned weights\n",
        "BertConfig.from_pretrained(\"bert-base-cased\", num_hidden_layers=10)\n",
        "BertModel(BertConfig)"
      ],
      "metadata": {
        "id": "FbnvxKxLylzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Tokenizers"
      ],
      "metadata": {
        "id": "mxEYkw8_uW8n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5-1. Tokenizers convert raw text to numbers called \"encoding\""
      ],
      "metadata": {
        "id": "Tk3UgEe5uev2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformer import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "seq1 = \"what's it like living in Ireland\"\n",
        "\n",
        "print(tokenizer(seq1)) # this is actually two steps as shown in the following:\n",
        "\n",
        "tokens = tokenizer.tokenize(seq1) # step 1. to see the tokenized seq\n",
        "print(tokens)\n",
        "\n",
        "print(tokenizer.convert_tokens_to_ids(tokens)) # step 2. convert to input IDs"
      ],
      "metadata": {
        "id": "XxEvFDvpumNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq2 = \"how's it like living in Korea\"\n",
        "\n",
        "tokens = tokenizer.tokenize(seq1)\n",
        "print(tokens)\n",
        "\n",
        "print(tokenizer.convert_tokens_to_ids(tokens))"
      ],
      "metadata": {
        "id": "0-I7Hr60vDv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Decoding is the opposite of encoding: converts vocabulary indices (input IDs) to raw text (tokens)\n",
        "# This behavior will be very useful for models that predict new text (e.g., text generation from a promt, seq-to-seq problems like translation or summarization)\n",
        "tokenizer.decode([1293, 112, 188, 1122, 1176, 1690, 1107, 3577])"
      ],
      "metadata": {
        "id": "HdrNtHKJvL67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Handling Multiple Sequences"
      ],
      "metadata": {
        "id": "VtQ-NtKJvkI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "## 1. Choose tokenizer model\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "## 2. Define Tokenizer based on defined checkpoint in 1.\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "## 3. Define sentiment analysis model => will be task-specific\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "## 4. Make seq test case\n",
        "seq = \"I've never been to Ireland. I'ld like to go someday\"\n",
        "\n",
        "## 5. Apply tokenizer defined in 2.\n",
        "\n",
        "## 6. Convert tokens to numbers (input IDs)\n",
        "\n",
        "## 7.\n",
        "input_ids = torch.tensor([ids])\n",
        "print(input_ids)\n",
        "\n",
        "## Logit\n",
        "outputs = model(input_ids)\n",
        "\n",
        "## Convert logit to probability using softmax\n",
        "preds = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "q7vqSSdOvnzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "## 1. Choose tokenizer model\n",
        "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
        "\n",
        "## 2. Define Tokenizer based on defined checkpoint in 1.\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "## 3. Define sentiment analysis model => will be task-specific\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "\n",
        "## 4. Make seq test case\n",
        "seqs = [\"I've been waiting very long to finally meet you\", \"I missed you so much\"]\n",
        "\n",
        "## 5. Apply tokenizer defined in 2.\n",
        "tokens = tokenizer(seqs, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "output = model(**tokens)\n",
        "print(output)\n",
        "\n",
        "preds = torch.nn.functional.softmax(output.logits, dim=-1)\n",
        "print(preds)"
      ],
      "metadata": {
        "id": "iG8j4B-1wTbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Fine-Tuning a Pretrained Model"
      ],
      "metadata": {
        "id": "-9IBFqKeiOlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7-1. Preprocessing"
      ],
      "metadata": {
        "id": "aNwIQuW1ZUaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## goal: from a pair of sentences, the model can tell if they are a paraphrase\n",
        "from datasets import load_dataset\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "raw_datasets"
      ],
      "metadata": {
        "id": "cp6GpydYZbII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_dataset = raw_datasets[\"train\"]\n",
        "raw_train_dataset[0]"
      ],
      "metadata": {
        "id": "ZltqhXbdZlJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_train_dataset.features"
      ],
      "metadata": {
        "id": "PD2otzMMZnh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Caution: Because we need a pair of sentences, we construct the tokenizer as follows\n",
        "# However, this stores the data in RAM which will make the notebook crash\n",
        "# Instead, we use the \"datasets\" from the Datasets library which are Apache Arrow files\n",
        "# stored on the disk\n",
        "# Exception: when trained on a TPU. it prefers fixed shapes, even when that requires extra padding\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "## this method works but it will take significantly more time as the data is stored in RAM\n",
        "# tokenized_dataset = tokenizer(\n",
        "#     raw_datasets[\"train\"][\"sentence1\"],\n",
        "#     raw_datasets[\"train\"][\"sentence2\"],\n",
        "#     padding=True,\n",
        "#     truncation=True,\n",
        "# )\n",
        "\n",
        "## correct method\n",
        "# to keep the data as a \"dataset\", we use Dataset.map()\n",
        "# * Note: we skip padding at this step as it's inefficient to apply padding to the entire data\n",
        "# with max length => instead we do this at the batched level aka \"dynamic padding\"\n",
        "# * we implement dynamic padding by using a \"collate function\" which puts together samples inside a batch\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "FJWhOvfGZ3QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets['train'][0]"
      ],
      "metadata": {
        "id": "r7QchfbTb-Kb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "data_collator"
      ],
      "metadata": {
        "id": "R1uR_vH0gUsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets[\"train\"].column_names"
      ],
      "metadata": {
        "id": "YdH4XnvghKi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## grab some sample => assume this is our first batch\n",
        "samples = tokenized_datasets[\"train\"][:5]\n",
        "\n",
        "## remove idx, sentence1, sentence2 as we can't create tensors w/ strings\n",
        "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
        "\n",
        "## as you can see the length vary\n",
        "# if dynamic padding is applied to this batch it should all be padded to the max length which is 67\n",
        "[len(x) for x in samples[\"input_ids\"]]"
      ],
      "metadata": {
        "id": "joCIcv_EwJMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## check if dynamic padding max length is 67 for this batch\n",
        "batch = data_collator(samples)\n",
        "{k: v.shape for k, v in batch.items()}"
      ],
      "metadata": {
        "id": "crEj8ziCiDcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7-2. Fine-tuning a model with the Trainer API"
      ],
      "metadata": {
        "id": "mEWgiWko0gkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets"
      ],
      "metadata": {
        "id": "918VEo_j03z3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## from previous section 7-1\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "## apply tokenizer keeping the data as a \"dataset\" using the Dataset.map() method\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "## dynamic padding using DataCollatorWithPadding\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "GrdK1w-t0o0o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator"
      ],
      "metadata": {
        "id": "VI5Tj0-cv003"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7-2-1. Method 1: w/o using evaluation metrics during training"
      ],
      "metadata": {
        "id": "G_Iu1TenBmMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 1\n",
        "## Define model setup using TrainingArguments\n",
        "# contains all the hyperparameters\n",
        "# here the only argument we need to change is a directory where the trained model will be saved\n",
        "from transformers import TrainingArguments\n",
        "training_args = TrainingArguments(\"test-trainer\")\n",
        "\n",
        "# list of all the avaiable parameters for training\n",
        "training_args"
      ],
      "metadata": {
        "id": "vsAmmqURx06y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 2\n",
        "## Define model\n",
        "## Caveat: we get a warning as the model we chose (BERT) hasn't been trained on classifying\n",
        "# pairs of sentences\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "model"
      ],
      "metadata": {
        "id": "j3agaVVdyzpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 3\n",
        "## Put all the objects constructed up to now by defining a Trainer\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator, # default data_collator used by the Trainer is DataCollatorWithPadding\n",
        "    processing_class=tokenizer,\n",
        ")\n",
        "trainer"
      ],
      "metadata": {
        "id": "YwpIFtoOy8Mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 4\n",
        "## Fine-tune\n",
        "# takes too long => change parameter and retry\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "zJanpvjJzJ_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 5\n",
        "## Evaluation\n",
        "predictions = trainer.predict(tokenized_datasets[\"validation\"])\n",
        "\n",
        "# returns a named tuple: predictions, label_ids, metrics (loss, execution time etc)\n",
        "# (408, 2)\n",
        "# (408,)\n",
        "print(predictions.predictions.shape, predictions.label_ids.shape)\n",
        "\n",
        "## why are we doing argmax()\n",
        "# ? revisit as not fully understood why argmax is needed\n",
        "# to match the predictions with our labels, we take the max logit for each prediction\n",
        "# to know which of the two classes were predicted\n",
        "import numpy as np\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "\n",
        "# we now compare preds to the labels\n",
        "import evaluate\n",
        "metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "metric.compute(predictions=preds, references=predictions.label_ids)"
      ],
      "metadata": {
        "id": "HZ45L2vP1PaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7-2-2. Method 2: by using evaluation metrics during training"
      ],
      "metadata": {
        "id": "qPEAhyR8COvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    metric = evaluate.load(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "## evaluation_strategy=\"epoch\": tells the Trainer to evaluate at the end of every epoch\n",
        "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "-_h-iR1D4cly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7-3. Full Training using PyTorch"
      ],
      "metadata": {
        "id": "khqXLX4uGoN6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
        "\n",
        "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
        "checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)\n",
        "\n",
        "## apply tokenizer keeping the data as a \"dataset\" using the Dataset.map() method\n",
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "EKXNCpwD4grp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence1\", \"sentence2\", \"idx\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "tokenized_datasets[\"train\"].column_names"
      ],
      "metadata": {
        "id": "d3KsOZOhG1ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"], batch_size=8, collate_fn=data_collator\n",
        ")"
      ],
      "metadata": {
        "id": "EZmKhgBCG5xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_dataloader:\n",
        "    break\n",
        "{k: v.shape for k, v in batch.items()}"
      ],
      "metadata": {
        "id": "n27ZKTq4G8IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)"
      ],
      "metadata": {
        "id": "kvEJA6v5G-9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**batch)\n",
        "print(outputs.loss, outputs.logits.shape)"
      ],
      "metadata": {
        "id": "REx2gankHCCb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}