{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP7EVFGaSY2awLHQpx5P6Y2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xpdlaldam/nlp/blob/master/Hugging%20Face/audio_analysis/speech_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "K5--AxGGp1p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U datasets gradio evaluate transformers[sentencepiece]"
      ],
      "metadata": {
        "id": "Rm65NrIEsZxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U datasets"
      ],
      "metadata": {
        "id": "iiRyQ3lYyO90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from datasets import load_dataset\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "MUNpWVrup5NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRpHOgKQn1Sg"
      },
      "outputs": [],
      "source": [
        "# load model and tokenizer\n",
        "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "\n",
        "# load dummy dataset and read soundfiles\n",
        "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"superb\", name=\"asr\", split=\"test\")"
      ],
      "metadata": {
        "id": "sd_9aNM6zCax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import list_datasets\n",
        "print([dataset.id for dataset in list_datasets()])"
      ],
      "metadata": {
        "id": "1EtN1wSHykci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds = load_dataset(\"PolyAI/minds14\", name=\"ko-KR\", split=\"train\")\n",
        "minds"
      ],
      "metadata": {
        "id": "vnZUMQHO0hvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds = load_dataset(\"PolyAI/minds14\", name=\"en-GB\", split=\"train\")\n",
        "minds"
      ],
      "metadata": {
        "id": "ukNbyRyf1tW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = minds.features[\"intent_class\"].int2str\n",
        "id2label(minds[0][\"intent_class\"])"
      ],
      "metadata": {
        "id": "7MBvV7sh1S34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stream data instead of downloading the full dataset\n",
        "dataset = load_dataset(\"ymoslem/EUbookshop-Speech-Irish\", split=\"train\", streaming=True)\n",
        "\n",
        "# Take the first 5 samples\n",
        "small_sample = [next(iter(dataset)) for _ in range(5)]\n",
        "small_sample"
      ],
      "metadata": {
        "id": "tB7xbaoD2iE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id2label = minds.features[\"intent_class\"].int2str\n",
        "id2label(minds[0][\"intent_class\"])"
      ],
      "metadata": {
        "id": "45BcOEzF2s78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds['intent_class']"
      ],
      "metadata": {
        "id": "1Upa42YS31iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_audio():\n",
        "    example = minds[0]\n",
        "    audio = example[\"audio\"]\n",
        "    return (\n",
        "        audio[\"sampling_rate\"], # Hz\n",
        "        audio[\"array\"], # contains the sound represented in numbers in an array\n",
        "    ), id2label(example[\"intent_class\"])"
      ],
      "metadata": {
        "id": "Pr3dPgzn2GuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    with gr.Column():\n",
        "        for _ in range(1):\n",
        "            audio, label = generate_audio()\n",
        "            output = gr.Audio(audio, label=label)\n",
        "\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "m6zQepy21JyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mJmwQV2I4pQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\").input_values  # Batch size 1\n",
        "\n",
        "# retrieve logits\n",
        "logits = model(input_values).logits\n",
        "\n",
        "# take argmax and decode\n",
        "predicted_ids = torch.argmax(logits, dim=-1)\n",
        "transcription = processor.batch_decode(predicted_ids)"
      ],
      "metadata": {
        "id": "ycVKqs9Wxm4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from datasets import load_dataset, Audio\n",
        "\n",
        "# Load dataset in streaming mode\n",
        "streaming_data = load_dataset(\"ymoslem/EUbookshop-Speech-Irish\", split=\"train\", streaming=True)\n",
        "stream_iter = iter(streaming_data)\n",
        "\n",
        "# Set up the Audio decoder (to decode audio directly from the stream)\n",
        "audio_decoder = Audio()\n",
        "\n",
        "# Function to generate audio and label from the stream\n",
        "def generate_audio():\n",
        "    # Get the next sample from the stream\n",
        "    example = next(stream_iter)\n",
        "\n",
        "    # Decode the audio\n",
        "    audio = audio_decoder.decode_example(example[\"audio\"])\n",
        "    audio_array = audio[\"array\"]\n",
        "    sampling_rate = audio[\"sampling_rate\"]\n",
        "\n",
        "    # Get English translation as label\n",
        "    label = example.get(\"translation\", {}).get(\"en\", \"No English translation available\")\n",
        "\n",
        "    return (audio_array, sampling_rate), label\n",
        "\n",
        "# Build the Gradio UI\n",
        "with gr.Blocks() as demo:\n",
        "    with gr.Column():\n",
        "        # Gradio components\n",
        "        audio_component = gr.Audio(label=\"Irish Speech Audio\", type=\"numpy\")\n",
        "        label_component = gr.Textbox(label=\"English Translation\")\n",
        "\n",
        "        def update():\n",
        "            return generate_audio()\n",
        "\n",
        "        # Button to trigger the next sample\n",
        "        gr.Button(\"Get Next Sample\").click(fn=update, inputs=[], outputs=[audio_component, label_component])\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "zJlYCxzD4rdC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}