{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPunyCxTEoO+k8cTn9fWVzi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xpdlaldam/nlp/blob/master/nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLyTnpIZu5C_"
      },
      "outputs": [],
      "source": [
        "### Stemming & Lemmatization\n",
        "## Stemming is very crude as it just chops off the end of the word => the result is not necessarily a real word ex) better -> better\n",
        "## Lemmatization is more sophisticated as it uses actual rules of language => the true root word will be returned ex) better -> good\n",
        "## Parts-of-Speech (POS) matters as \"going\" can be a noun but when the root form is \"go\" it is not a noun\n",
        "\n",
        "### Bag-of-Words (BoW)\n",
        "## counts word frequencies\n",
        "## unordered\n",
        "## simpler model than TF-IDF but useful if computaional speed is importat\n",
        "\n",
        "### TF-IDF\n",
        "## considers both word frequency and rarity across all documents\n",
        "\n",
        "### Tokenization\n",
        "## the process of breaking down text into smaller units, called tokens\n",
        "## text.split(): ex) I like cats => [I, \"like\", \"cats\"]\n",
        "## punctuations like . or ? can matter depending on your task ex) I like cats? => [I, \"like\", \"cats\", \"?\"]\n",
        "## note: scikit-learn's CountVectorizer ignores punctuation\n",
        "## casing: cat and Cat are identical => CountVectorizer(lowercase=True)\n",
        "## accents ex) resume => CountVectorizer(strip_accents=True)\n",
        "\n",
        "## Types of tokenization\n",
        "# 1. word-based tokenization: takes up too much space but contains more information => CountVectorizer(analyzer=\"word\")\n",
        "# 2. character-based tokenization: takes up less space but contains less information => CountVectorizer(analyzer=\"char\")\n",
        "# 3. subword-based tokenization: middle ground b/w word and character-based\n",
        "# ex) we want \"walking\" to be split into \"walk\" + \"ing\" o.w. \"walking\" and \"walk\" will be recognized as\n",
        "# different words\n",
        "\n",
        "### Steps of a typical NLP analysis\n",
        "## 1. get the text\n",
        "## 2. tokenize the text\n",
        "## 3. stopwords, stemming / lemmatization\n",
        "## 4. map tokens to integers\n",
        "## 5. convert text into count vectors / TF-IDF\n",
        "## 6. Do ML task\n",
        "\n",
        "### Markov Property\n",
        "## x_t only depends on x_t-1 or\n",
        "## p(x_t|x_t-1, x_t-2, ...) = p(x_t|x_t-1) or\n",
        "## x_t is independent of x_t-2, x_t-3, ...\n",
        "## use case in NLP:\n",
        "  # from 2000 most common English words\n",
        "  # we want to predict the 10th word\n",
        "  # from the previous 9 words in a sentence\n",
        "  # our model is p(x_10|x_9, x_8, ..., x_1)\n",
        "  # each x_i has 2_000 possible words, hence\n",
        "  # total probabilities to estimate is 2000^10\n",
        "## we assume the Markov property holds,\n",
        "## even when it does not as the next word\n",
        "## doesn't always depend on the previous word\n",
        "  # ex) I like eggs and ham\n",
        "  # Does \"ham\" only depend on \"and\"? no\n",
        "\n",
        "### Markov Model\n",
        "## state distribution p(s_t) = state distribution w/ length M vector\n",
        "# ex) prob it will rain on Sunday = p(s_sunday=rainy)\n",
        "\n",
        "## state transition matrix:\n",
        "# A_ij = p(s_t = j|s_t-1 = i), for all i = 1,...,M and j = 1,...,M\n",
        "# if we apply Markov model to NLP, for the first word in a sentence\n",
        "# there is no previous state, hence to quantify the probability\n",
        "# of the first state in a sequence, we use the initial state distribution\n",
        "# pi_i = p(s_1 = i) for i=1,...,M\n",
        "\n",
        "## How to estimate A and pi\n",
        "# A hat sub ij = count(i => j) / count(i)\n",
        "# ex) prob of seeing the word \"cat\" following the word \"the\"\n",
        "# i.e. \"the\" => \"cat\"\n",
        "# i.e. count(\"the cat\") / count(\"the\")\n",
        "\n",
        "## we can use a Markov model for text classification\n",
        "# note that text classification is supervised learning\n",
        "# but Markov models are unsupervised\n",
        "# solution is to apply Bayes' rule\n",
        "# ex) train a Markov model by feeding poems by poet A\n",
        "# train a Markov model by feeding poems by poet B\n",
        "# for a new unknown text we want to know which\n",
        "# poet wrote the poem i.e.\n",
        "# p(x|poet=A)\n",
        "# p(x|poet=B)\n",
        "# we choose the highest probability\n",
        "# in other words, we have p(poem|poet)\n",
        "# but we want p(poet|poem)\n",
        "# argmax_k p(class=k|data)\n",
        "# p(poet|poem) = p(poem|poet=k)p(poet=k) / p(poem)\n",
        "# k* = argmax_k p(poem|poet=k)p(poet=k) / p(poem)\n",
        "# proportional to\n",
        "# = argmax_k p(poem|poet=k)p(poet=k)\n",
        "# = argmax_k logp(poem|poet=k) + logp(poet=k)\n",
        "# proportional to\n",
        "# = argmax_k logp(poem|poet=k) (assuming p(poet) is uniform)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"mice\") # returns \"mouse\"\n",
        "\n",
        "lemmatizer.lemmatize(\"going\", pos=wordnet.NOUN) # returns \"going\"\n",
        "lemmatizer.lemmatize(\"going\", pos=wordnet.VERB) # returns 'go'"
      ],
      "metadata": {
        "id": "TwlHNxrnSM0E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14.Stemming & Lemmatization Demo 6/16/2024"
      ],
      "metadata": {
        "id": "s7ercQtwFfQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download(\"wordnet\")\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "tjJtf0sKBrU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter = PorterStemmer()"
      ],
      "metadata": {
        "id": "zNyXf6RXB8fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter.stem(\"walking\")"
      ],
      "metadata": {
        "id": "5J4O8wwyCAOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter.stem(\"walked\")"
      ],
      "metadata": {
        "id": "qxfKbEGcCW6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "porter.stem(\"ran\") # wrong stemming"
      ],
      "metadata": {
        "id": "Am74gDgxET6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Lemmatization is more sophisticated than stemming\".split()\n",
        "for token in sentence:\n",
        "  # print(porter.stem(token), end=\" \") # end = \" \" is to print the result in a single line\n",
        "  print(porter.stem(token))"
      ],
      "metadata": {
        "id": "ap-WUPb2EiAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "aFgCVhhtFOFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"walking\") # default: pos=wordnet.NOUN"
      ],
      "metadata": {
        "id": "nh6xE5hMFyje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"walking\", pos=wordnet.VERB)"
      ],
      "metadata": {
        "id": "xwme96ZlF5hd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"ran\", pos=wordnet.VERB)"
      ],
      "metadata": {
        "id": "S5SMB__mGGSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"was\", pos=wordnet.VERB)"
      ],
      "metadata": {
        "id": "j9VVamztGcFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer.lemmatize(\"better\", pos=wordnet.ADJ)"
      ],
      "metadata": {
        "id": "oK9WarfqGouV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "  if treebank_tag.startswith(\"J\"):\n",
        "    return wordnet.ADJ\n",
        "  elif treebank_tag.startswith(\"V\"):\n",
        "    return wordnet.VERB\n",
        "  elif treebank_tag.startswith(\"N\"):\n",
        "    return wordnet.NOUN\n",
        "  elif treebank_tag.startswith(\"R\"):\n",
        "    return wordnet.ADJ\n",
        "  else:\n",
        "    return wordnet.NOUN"
      ],
      "metadata": {
        "id": "rZB7A7xrGrvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"averaged_perceptron_tagger\")"
      ],
      "metadata": {
        "id": "BixkIeDOHb6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Donald Trump has a devoted following\".split()\n",
        "words_and_tags = nltk.pos_tag(sentence)\n",
        "words_and_tags"
      ],
      "metadata": {
        "id": "IWraGpZxHfxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, tag in words_and_tags:\n",
        "  lemma = lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag))\n",
        "  print(lemma, end=\" \")"
      ],
      "metadata": {
        "id": "WfOyv-6xHzd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The cat was following the bird as it flew by\".split()\n",
        "words_and_tags = nltk.pos_tag(sentence)\n",
        "words_and_tags"
      ],
      "metadata": {
        "id": "3FOoW9UJIGC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word, tag in words_and_tags:\n",
        "  lemma = lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag))\n",
        "  print(lemma, end=\" \")"
      ],
      "metadata": {
        "id": "lGTOUUogIQKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. Count Vectorizer"
      ],
      "metadata": {
        "id": "U7LDv_H_OP7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "from nltk.corpus import wordnet"
      ],
      "metadata": {
        "id": "SyXsSQoAOV5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## download data for nltk\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "jm6GanLDS06b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/shivamkushwaha/bbc-full-text-document-classification\n",
        "!gsutil cp gs://dataset-uploader/bbc/bbc-text.csv ."
      ],
      "metadata": {
        "id": "u1KlfXBPTR3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"bbc-text.csv\")"
      ],
      "metadata": {
        "id": "Q9-9x_GJT4Fg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "uFO_okAIULzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = df['text']\n",
        "labels = df['category']"
      ],
      "metadata": {
        "id": "oUugkj-eUanr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels.hist(figsize=(10, 5))"
      ],
      "metadata": {
        "id": "nJ7FpXBtUyhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_train, inputs_test, Ytrain, Ytest = train_test_split(\n",
        "    inputs, labels, random_state=123)"
      ],
      "metadata": {
        "id": "lWD3i4Z9UziH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer()"
      ],
      "metadata": {
        "id": "pPhcr8W7VAqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain = vectorizer.fit_transform(inputs_train)\n",
        "Xtest = vectorizer.transform(inputs_test)"
      ],
      "metadata": {
        "id": "BcdoHctiVDty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain"
      ],
      "metadata": {
        "id": "JrEQ5NhvVWjq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(Xtrain != 0).sum()"
      ],
      "metadata": {
        "id": "Y4jYuC4NVsWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what percentage of values are non-zero?\n",
        "# < 1% => most values are non-zeros\n",
        "# thus, makes sense to use a sparse matrix\n",
        "(Xtrain != 0).sum() / np.prod(Xtrain.shape)"
      ],
      "metadata": {
        "id": "vHD-UgfUVka0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultinomialNB()\n",
        "model.fit(Xtrain, Ytrain)\n",
        "print(\"train score:\", model.score(Xtrain, Ytrain))\n",
        "print(\"test score:\", model.score(Xtest, Ytest))"
      ],
      "metadata": {
        "id": "b3mVYoGVV7JH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using stopwords (default: stopwords are not removed)\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "Xtrain = vectorizer.fit_transform(inputs_train)\n",
        "Xtest = vectorizer.transform(inputs_test)\n",
        "model = MultinomialNB()\n",
        "model.fit(Xtrain, Ytrain)\n",
        "print(\"train score:\", model.score(Xtrain, Ytrain))\n",
        "print(\"test score:\", model.score(Xtest, Ytest))"
      ],
      "metadata": {
        "id": "cceZ8r7tWYNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_wordnet_pos(treebank_tag):\n",
        "  if treebank_tag.startswith('J'):\n",
        "    return wordnet.ADJ\n",
        "  elif treebank_tag.startswith('V'):\n",
        "    return wordnet.VERB\n",
        "  elif treebank_tag.startswith('N'):\n",
        "    return wordnet.NOUN\n",
        "  elif treebank_tag.startswith('R'):\n",
        "    return wordnet.ADV\n",
        "  else:\n",
        "    return wordnet.NOUN"
      ],
      "metadata": {
        "id": "tw5Qm90LW75x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LemmaTokenizer:\n",
        "  def __init__(self):\n",
        "    self.wnl = WordNetLemmatizer()\n",
        "\n",
        "  # call an object as if it were a function\n",
        "  def __call__(self, doc):\n",
        "    tokens = word_tokenize(doc)\n",
        "    words_and_tags = nltk.pos_tag(tokens)\n",
        "    return [self.wnl.lemmatize(word, pos=get_wordnet_pos(tag)) \\\n",
        "            for word, tag in words_and_tags]"
      ],
      "metadata": {
        "id": "YJSxprYVXEf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with lemmatization\n",
        "vectorizer = CountVectorizer(tokenizer=LemmaTokenizer())\n",
        "Xtrain = vectorizer.fit_transform(inputs_train)\n",
        "Xtest = vectorizer.transform(inputs_test)\n",
        "model = MultinomialNB()\n",
        "model.fit(Xtrain, Ytrain)\n",
        "print(\"train score:\", model.score(Xtrain, Ytrain))\n",
        "print(\"test score:\", model.score(Xtest, Ytest))"
      ],
      "metadata": {
        "id": "luaz45mfXeBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StemTokenizer:\n",
        "  def __init__(self):\n",
        "    self.porter = PorterStemmer()\n",
        "  def __call__(self, doc):\n",
        "    tokens = word_tokenize(doc)\n",
        "    return [self.porter.stem(t) for t in tokens]"
      ],
      "metadata": {
        "id": "5LDdKitNX399"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with stemming\n",
        "vectorizer = CountVectorizer(tokenizer=StemTokenizer())\n",
        "Xtrain = vectorizer.fit_transform(inputs_train)\n",
        "Xtest = vectorizer.transform(inputs_test)\n",
        "model = MultinomialNB()\n",
        "model.fit(Xtrain, Ytrain)\n",
        "print(\"train score:\", model.score(Xtrain, Ytrain))\n",
        "print(\"test score:\", model.score(Xtest, Ytest))"
      ],
      "metadata": {
        "id": "XNsYR1AiX7IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_tokenizer(s):\n",
        "  return s.split()"
      ],
      "metadata": {
        "id": "zuthJ6gxYHcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# string split tokenizer\n",
        "vectorizer = CountVectorizer(tokenizer=simple_tokenizer)\n",
        "Xtrain = vectorizer.fit_transform(inputs_train)\n",
        "Xtest = vectorizer.transform(inputs_test)\n",
        "model = MultinomialNB()\n",
        "model.fit(Xtrain, Ytrain)\n",
        "print(\"train score:\", model.score(Xtrain, Ytrain))\n",
        "print(\"test score:\", model.score(Xtest, Ytest))"
      ],
      "metadata": {
        "id": "EwSJXE2vYMvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF Theory"
      ],
      "metadata": {
        "id": "sPZQxjkjjRMq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### concept\n",
        "# Term Frequency - Inverse Document Frequency\n",
        "# to avoid stopwords so that it doesn't over represent the documnet\n",
        "# term frequency / document frequency\n",
        "\n",
        "### tfidf(t, d) = tf(t, d) * idf(t)\n",
        "## tf(t, d)\n",
        "# number of times a term appears in an document\n",
        "# t is the term count and d is the ith document ex) document 1 has the word \"hi\" 3 times and document 2 has the word \"bye\" 4 times\n",
        "# the t is the only argument in idf as the document count only depends on the term t\n",
        "# i.e. tf(t, d) = # of times t appears in d => same result by using CountVectorizer; if we have V unique terms and N documents the dimension will be N by V\n",
        "\n",
        "## idf(t) = log(N/N(t)), where N = total # of documnets; N(t) = # of documents term t appears in\n",
        "# a measure of how rare a term is across all documents\n",
        "# why log? nomotinic: if N/N(t) increases, so will its log\n",
        "# TF-IDF decreases if t appears in more documents\n",
        "# to squash big numbers ex) if N = 10M and N(t) = 1, then TF-IDF explodes to 1M but with log it goes down to 13.8"
      ],
      "metadata": {
        "id": "Q4HZI4FRjT21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF Recommender System"
      ],
      "metadata": {
        "id": "UCCfDasz6pM3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/tmdb/tmdb-movie-metadata\n",
        "!wget https://lazyprogrammer.me/course_files/nlp/tmdb_5000_movies.csv"
      ],
      "metadata": {
        "id": "w21_ycK76taG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances"
      ],
      "metadata": {
        "id": "MDONU8Lc8GgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('tmdb_5000_movies.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "zeYFxQ-b8xq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['genres'].unique()"
      ],
      "metadata": {
        "id": "tGYTSAzT83iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['keywords'].unique()"
      ],
      "metadata": {
        "id": "jOAqQXhg9zVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "j = json.loads(df.iloc[0]['genres'])\n",
        "j"
      ],
      "metadata": {
        "id": "XPjW1V_699YW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[jj['name'] for jj in j]"
      ],
      "metadata": {
        "id": "nd9Pk7BX-Gx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[jj['name'].split() for jj in j] # splits into words w/ each of them stored in a list of lists"
      ],
      "metadata": {
        "id": "e_3LcNry-n8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[''.join(jj['name'].split()) for jj in j] # remove white space"
      ],
      "metadata": {
        "id": "pi3kLpCC_Esr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "' '.join(''.join(jj['name'].split()) for jj in j) # make all genres in a single string"
      ],
      "metadata": {
        "id": "0CpT21k_CSEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the relevant data for each movie into a single string\n",
        "# to be ingested by TfidfVectorizer\n",
        "def genres_and_keywords_to_string(row):\n",
        "  genres = json.loads(row['genres'])\n",
        "  genres = ' '.join(''.join(j['name'].split()) for j in genres)\n",
        "\n",
        "  keywords = json.loads(row['keywords'])\n",
        "  keywords = ' '.join(''.join(j['name'].split()) for j in keywords)\n",
        "  return \"%s %s\" % (genres, keywords)"
      ],
      "metadata": {
        "id": "UXC76vmFCkOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['string'] = df.apply(genres_and_keywords_to_string, axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ypD-ddaMDS8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a tf-idf vectorizer object\n",
        "tfidf = TfidfVectorizer(max_features=2000) # max_features: limits the number of columns in the final matrix i.e. discards less frequent terms"
      ],
      "metadata": {
        "id": "il-eTiq-DUWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a data matrix from the overviews\n",
        "X = tfidf.fit_transform(df['string'])\n",
        "X"
      ],
      "metadata": {
        "id": "O96_ABnrDxHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate a mapping from movie title -> index (in df)\n",
        "# why are we doing this? b/c our X matrix doesn't tell you which movie. It's just numbers so we need to have a mapping that tells which movie it is\n",
        "movie2idx = pd.Series(df.index, index=df['title'])\n",
        "movie2idx"
      ],
      "metadata": {
        "id": "znIQnE4oEx0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = movie2idx['Avatar']\n",
        "idx"
      ],
      "metadata": {
        "id": "Asf2TvZLFJta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = X[idx]\n",
        "query"
      ],
      "metadata": {
        "id": "9BSa9oszFTSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query.toarray()"
      ],
      "metadata": {
        "id": "d4YEs43YFXbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute similarity between query and every vector in X\n",
        "# shape will be 1 (query.shape[0]) by 4_803 (X.shape[0])\n",
        "# 0 means there are no common terms b/w two movies => dot product = 0\n",
        "scores = cosine_similarity(query, X)\n",
        "scores"
      ],
      "metadata": {
        "id": "tCzU5LiFGt5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# currently the array is 1 x N, make it just a 1-D array\n",
        "scores = scores.flatten()"
      ],
      "metadata": {
        "id": "4PJhn1EWHfHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(scores) # the spike at 1 is the query movie as the cosine similarity b/w two identical vector is 1"
      ],
      "metadata": {
        "id": "OTnOVF2oHjO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reason for doing argsort: we don't care about the scores we only care about the ranks\n",
        "# -scores: sort by descending order\n",
        "(-scores).argsort()"
      ],
      "metadata": {
        "id": "W0zmxYrHII1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scores[(-scores).argsort()]"
      ],
      "metadata": {
        "id": "zlAvXdRoIsWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# interpretation: most movies are unrelated to the query movie\n",
        "plt.plot(scores[(-scores).argsort()])"
      ],
      "metadata": {
        "id": "ejQZpxoGImmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get top 5 matches\n",
        "# exclude self (similarity between query and itself yields max score)\n",
        "recommended_idx = (-scores).argsort()[1:6] # 0 is the query movie itself so start from 1\n",
        "\n",
        "# convert indices back to titles\n",
        "df['title'].iloc[recommended_idx]"
      ],
      "metadata": {
        "id": "JdpFU_uvJGL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a function that generates recommendations\n",
        "def recommend(title):\n",
        "  # get the row in the dataframe for this movie\n",
        "  idx = movie2idx[title]\n",
        "  if type(idx) == pd.Series: # if it's a pd.Series, it's the same title\n",
        "    idx = idx.iloc[0]\n",
        "\n",
        "  # calculate the pairwise similarities for this movie\n",
        "  query = X[idx]\n",
        "  scores = cosine_similarity(query, X)\n",
        "\n",
        "  # currently the array is 1 x N, make it just a 1-D array\n",
        "  scores = scores.flatten()\n",
        "\n",
        "  # get the indexes of the highest scoring movies\n",
        "  # get the first K recommendations\n",
        "  # don't return itself!\n",
        "  recommended_idx = (-scores).argsort()[1:6]\n",
        "\n",
        "  # return the titles of the recommendations\n",
        "  return df['title'].iloc[recommended_idx]"
      ],
      "metadata": {
        "id": "1nZYFhRoJLzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommend(title='The Dark Knight Rises')"
      ],
      "metadata": {
        "id": "J9zoyvOhKJY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word-to-Index mapping"
      ],
      "metadata": {
        "id": "JvA6fCgkzcD9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "HBFIgjho3cqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# word-to-index is needed to build TF-IDF from scratch, hence it is not needed for sklearn but for advanced NLP\n",
        "# such as deep nlp with pytorch, word2vec, RNNs, transformers etc\n",
        "\n",
        "# when converting documents to vectors, the data format is a document-term matrix, where\n",
        "# row = document and column = term (words)\n",
        "# ex) document 1: I like cats; document 2: I love cats; document 3: I love dogs\n",
        "data = {\n",
        "    'document': ['doc1', 'doc2', 'doc3'],\n",
        "    'I': [1, 1, 1],\n",
        "    'like': [1, 0, 0],\n",
        "    'cats': [1, 1, 0],\n",
        "    'love': [0, 1, 1],\n",
        "    'dogs': [0, 0, 1],\n",
        "}\n",
        "pd.DataFrame(data).set_index('document')"
      ],
      "metadata": {
        "id": "QxMko8KfKORG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF from scratch"
      ],
      "metadata": {
        "id": "AXx79msN8uP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.kaggle.com/shivamkushwaha/bbc-full-text-document-classification\n",
        "!wget -nc https://lazyprogrammer.me/course_files/nlp/bbc_text_cls.csv"
      ],
      "metadata": {
        "id": "5TLLLExX3NNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk import word_tokenize"
      ],
      "metadata": {
        "id": "JzmZLCkd9foz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "g9-Z-4dO9moE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('bbc_text_cls.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "0m56fbHX9qGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "word2idx = {} # dictionary to populate w/ word: index\n",
        "tokenized_docs = []\n",
        "\n",
        "# loops through rows in the 'text' column\n",
        "for doc in df['text'].head(1):\n",
        "\n",
        "  # doc is a collection of strings inside a list ex) \"Ad sales booost time warner profit...\"\n",
        "  # so doc.lower() makes all the individual strings lower case\n",
        "  # word_tokenize() splits doc smartly into words ex) ['ad', 'sales', 'boost', 'time', 'warner', 'profit']\n",
        "  words = word_tokenize(doc.lower()) # ex) ['ad', 'sales', 'boost', 'time', 'warner', 'profit']\n",
        "  # print(words)\n",
        "\n",
        "  doc_as_int = []\n",
        "  for word in words: # loops through all the individual words (separated by white space) from words\n",
        "    if word not in word2idx: # if it's a new word\n",
        "      word2idx[word] = idx # then add the new word to the dictionary word2idx\n",
        "      idx += 1 # we have to increase the number by 1 to make sure each new word gets assigned a new idx\n",
        "\n",
        "    # the keys from word2idx are all unique/new words\n",
        "    # however, this line is outside the \"if word not in word2idx\"\n",
        "    # hence, this line happens once all the unique new words are populated in word2idx\n",
        "    # doc_as_int adds all the values of the words from the populated word2idx that are in the entire row (word)\n",
        "    # so it can have duplicates if a word repeats such as \"to\" (was mapped as 16 so will have multiple 16's)\n",
        "    doc_as_int.append(word2idx[word])\n",
        "  tokenized_docs.append(doc_as_int)\n",
        "\n",
        "word2idx"
      ],
      "metadata": {
        "id": "ShTGxTLP-BIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_as_int"
      ],
      "metadata": {
        "id": "JYbUhAa2-D48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_docs"
      ],
      "metadata": {
        "id": "Wz-2aLtJ4iVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reverse word2idx\n",
        "idx2word = {v:k for k, v in word2idx.items()}\n",
        "idx2word"
      ],
      "metadata": {
        "id": "W5Moha-H5rRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of documents\n",
        "N = len(df['text'])\n",
        "N"
      ],
      "metadata": {
        "id": "QvfUDsuA6Zd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of words\n",
        "V = len(word2idx)\n",
        "V"
      ],
      "metadata": {
        "id": "0StcbJLj6d-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate term-frequency matrix\n",
        "# note: could have also used count vectorizer\n",
        "tf = np.zeros((N, V)) # number of documents by number of words\n",
        "tf.shape # (2_225, 235)"
      ],
      "metadata": {
        "id": "WUMZ1Cjj6gYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_docs"
      ],
      "metadata": {
        "id": "WHZR2-ERC4Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# populate term-frequency counts\n",
        "for i, doc_as_int in enumerate(tokenized_docs):\n",
        "  # print(i)\n",
        "  # print(i, doc_as_int)\n",
        "  for j in doc_as_int:\n",
        "    tf[i, j] += 1 # in document i count how many times the jth word appears"
      ],
      "metadata": {
        "id": "6-xPuYao68an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf"
      ],
      "metadata": {
        "id": "2EBc5OJt728a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'][0]"
      ],
      "metadata": {
        "id": "10Uj1JRb-CAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx2word"
      ],
      "metadata": {
        "id": "8ynuaI2s-OW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_as_int"
      ],
      "metadata": {
        "id": "ndEqNJW58nsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compute IDF\n",
        "# axis=0: sum over each word\n",
        "document_freq = np.sum(tf > 0, axis=0) # document frequency (shape = (V,))\n",
        "idf = np.log(N / document_freq)\n",
        "idf"
      ],
      "metadata": {
        "id": "fr1zWpuY_Vnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf = tf * idf\n",
        "tf_idf"
      ],
      "metadata": {
        "id": "XVzeQMCi_lWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pick a random document, show the top 5 terms (in terms of tf_idf score)\n",
        "i = np.random.choice(N) # random document index\n",
        "row = df.iloc[i] # the row of the chosen document\n",
        "print(\"Label:\", row['labels'])\n",
        "# print(\"Text:\", row['text'].split(\"\\n\", 1)[0])\n",
        "print(\"Text:\", row['text'])\n",
        "print(\"Top 5 terms:\")\n",
        "\n",
        "scores = tf_idf[i]\n",
        "\n",
        "# -scores orders in descending order\n",
        "# argsort: we don't care about the actual score instead we need the order\n",
        "indices = (-scores).argsort()\n",
        "\n",
        "for j in indices[:5]:\n",
        "  print(idx2word[j])"
      ],
      "metadata": {
        "id": "UITy3Rtw_xW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Embeddings"
      ],
      "metadata": {
        "id": "puaZeP8MOADE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### download pretrained models but this one takes too long\n",
        "## method 1\n",
        "# !wget -nc https://archive.org/download/google-news-vectors-negative-300.bin_202311/GoogleNews-vectors-negative300.bin.gz\n",
        "\n",
        "## method 2\n",
        "import gdown\n",
        "!gdown https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM"
      ],
      "metadata": {
        "id": "fYdYIgmMBK8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## unzip our gunzip file#\n",
        "!gunzip GoogleNews-vectors-negative300.bin.gz"
      ],
      "metadata": {
        "id": "A8dixFpZPpTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## KeyedVectors has the necessary AIPs for word embeddings\n",
        "from gensim.models import KeyedVectors"
      ],
      "metadata": {
        "id": "qaIe28bxQPJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Call the function load_word2vec_format passing in the pretrained word embeddings\n",
        "word_vectors = KeyedVectors.load_word2vec_format(\n",
        "  'GoogleNews-vectors-negative300.bin',\n",
        "  binary=True\n",
        ")"
      ],
      "metadata": {
        "id": "bhao90TYQ2-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_analogies(w1, w2, w3):\n",
        "  # w1 - w2 = ? - w3\n",
        "  # e.g. king - man = ? - woman\n",
        "  #      ? = +king +woman -man\n",
        "  r = word_vectors.most_similar(positive=[w1, w3], negative=[w2])\n",
        "  # print(\"%s - %s = %s - %s\" % (w1, w2, r[0][0], w3))\n",
        "  print(\"%s (%s) = %s - %s + %s\" % (r[0][0], round(r[0][1], 2), w1, w2, w3))\n",
        "  # print(r)"
      ],
      "metadata": {
        "id": "XE-uQEs7RjkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_analogies('king', 'man', 'woman')"
      ],
      "metadata": {
        "id": "36YHJL3ySe-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_analogies('france', 'paris', 'seoul')"
      ],
      "metadata": {
        "id": "IRS1jh6qTycU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nearest_neighbors(w):\n",
        "  r = word_vectors.most_similar(positive=[w])\n",
        "  print(\"neighbors of: %s\" % w)\n",
        "  for word, score in r:\n",
        "    print(\"\\t%s\" % word)"
      ],
      "metadata": {
        "id": "ZQjyQr_AVFWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbors('king')"
      ],
      "metadata": {
        "id": "OKs9IZ6UVJWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nearest_neighbors('seoul')"
      ],
      "metadata": {
        "id": "2QcryP7QVOsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise: download pretrained GloVe vectors from\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "id": "xvpUyH-qVtII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove*.zip"
      ],
      "metadata": {
        "id": "mJTiqfqlXfXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_glove_model(File):\n",
        "    print(\"Loading Glove Model\")\n",
        "    glove_model = {}\n",
        "    with open(File,'r') as f:\n",
        "        for line in f:\n",
        "            split_line = line.split()\n",
        "            word = split_line[0]\n",
        "            embedding = np.array(split_line[1:], dtype=np.float64)\n",
        "            glove_model[word] = embedding\n",
        "    print(f\"{len(glove_model)} words loaded!\")\n",
        "    return glove_model"
      ],
      "metadata": {
        "id": "CW5-kJaWYPU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_glove_model('glove.6B.50d.txt')"
      ],
      "metadata": {
        "id": "bNhMI8S3YRAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification"
      ],
      "metadata": {
        "id": "1ilWY57caia_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt\n",
        "!wget -nc https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_acDK6suanGZ",
        "outputId": "79087138-1be3-45ef-a05e-748030a74cc8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-02-07 02:33:13--  https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/edgar_allan_poe.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26622 (26K) [text/plain]\n",
            "Saving to: ‘edgar_allan_poe.txt’\n",
            "\n",
            "\redgar_allan_poe.txt   0%[                    ]       0  --.-KB/s               \redgar_allan_poe.txt 100%[===================>]  26.00K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-02-07 02:33:14 (22.4 MB/s) - ‘edgar_allan_poe.txt’ saved [26622/26622]\n",
            "\n",
            "--2025-02-07 02:33:14--  https://raw.githubusercontent.com/lazyprogrammer/machine_learning_examples/master/hmm_class/robert_frost.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 56286 (55K) [text/plain]\n",
            "Saving to: ‘robert_frost.txt’\n",
            "\n",
            "robert_frost.txt    100%[===================>]  54.97K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2025-02-07 02:33:14 (5.28 MB/s) - ‘robert_frost.txt’ saved [56286/56286]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "0dndGhWcgts0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!head edgar_allan_poe.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Py5_9Bwg1nF",
        "outputId": "dab25b54-99b4-4cc0-a8e7-b1db79f99843"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LO! Death hath rear'd himself a throne\n",
            "In a strange city, all alone,\n",
            "Far down within the dim west\n",
            "Where the good, and the bad, and the worst, and the best,\n",
            "Have gone to their eternal rest.\n",
            " \n",
            "There shrines, and palaces, and towers\n",
            "Are not like any thing of ours\n",
            "Oh no! O no! ours never loom\n",
            "To heaven with that ungodly gloom!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail edgar_allan_poe.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAVTbw5Vi-Yy",
        "outputId": "70453863-b450-43b4-cce4-10ed66979dd9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Of the old time entombed.\n",
            "\n",
            "And travellers now within that valley,\n",
            "Through the red-litten windows, see\n",
            "Vast forms that move fantastically\n",
            "To a discordant melody;\n",
            "While, like a rapid ghastly river,\n",
            "Through the pale door,\n",
            "A hideous throng rush out forever,\n",
            "And laugh --but smile no more."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## view entire file\n",
        "# !less edgar_allan_poe.txt\n",
        "\n",
        "##\n",
        "!wc edgar_allan_poe.txt # number of lines / number of words / number of characters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yS_l52e2jEn1",
        "outputId": "6834a937-4187-4ab0-9605-6f3b83d337c5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  796  4901 26622 edgar_allan_poe.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head robert_frost.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4C6LPN8FhPHY",
        "outputId": "bb2ae5db-07b8-4aca-f6e0-719fa2554959"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two roads diverged in a yellow wood,\n",
            "And sorry I could not travel both\n",
            "And be one traveler, long I stood\n",
            "And looked down one as far as I could\n",
            "To where it bent in the undergrowth; \n",
            "\n",
            "Then took the other, as just as fair,\n",
            "And having perhaps the better claim\n",
            "Because it was grassy and wanted wear,\n",
            "Though as for that the passing there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_files = [\n",
        "  'edgar_allan_poe.txt',\n",
        "  'robert_frost.txt',\n",
        "]"
      ],
      "metadata": {
        "id": "bFVFHJP3RecP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import islice\n",
        "\n",
        "for label, file in enumerate(input_files):\n",
        "  # print(file)\n",
        "  for line in open(file):\n",
        "  # for line in islice(file, 5):\n",
        "    # print(line)\n",
        "    # print(line.split())\n",
        "    # print(line.split()[:2])\n",
        "    print(\" \".join(line.split()[:2]))"
      ],
      "metadata": {
        "id": "XgVn7YR2Q0rf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Python3 code to demonstrate\n",
        "# translations using\n",
        "# maketrans() and translate()\n",
        "\n",
        "# specify to translate chars\n",
        "str1 = \"wy\"\n",
        "\n",
        "# specify to replace with\n",
        "str2 = \"gf\"\n",
        "\n",
        "# delete chars\n",
        "str3 = \"u\"\n",
        "\n",
        "# target string\n",
        "trg = \"weeksyourweeks\"\n",
        "\n",
        "# using maketrans() to\n",
        "# construct translate\n",
        "# table\n",
        "table = trg.maketrans(str1, str2, str3)\n",
        "\n",
        "# Printing original string\n",
        "print (\"The string before translating is : \", end =\"\")\n",
        "print (trg)\n",
        "\n",
        "# using translate() to make translations.\n",
        "print (\"The string after translating is : \", end =\"\")\n",
        "print (trg.translate(table))\n"
      ],
      "metadata": {
        "id": "6mgiG3AgXC0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# collect data into lists\n",
        "input_texts = []\n",
        "labels = []\n",
        "\n",
        "for label, file in enumerate(input_files):\n",
        "  print(f\"{file} corresponds to label {label}\")\n",
        "\n",
        "  for line in open(file):\n",
        "    line = line.rstrip().lower() # rstrip() removes white spaces from the end\n",
        "    if line: # b/c some lines are empty\n",
        "\n",
        "      # remove punctuation\n",
        "      # for now understanding this code is not necessary\n",
        "      line = line.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "      input_texts.append(line)\n",
        "      labels.append(label)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18IePbfilYoP",
        "outputId": "a737145e-056d-47b2-c256-ea45cb56f054"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "edgar_allan_poe.txt corresponds to label 0\n",
            "robert_frost.txt corresponds to label 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "P8r20BiWQycH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_texts[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyQp3sOXltiw",
        "outputId": "740bf537-d3f0-43a7-ca53-04158103936d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lo death hath reard himself a throne',\n",
              " 'in a strange city all alone',\n",
              " 'far down within the dim west',\n",
              " 'where the good and the bad and the worst and the best',\n",
              " 'have gone to their eternal rest']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbj85boAmSnF",
        "outputId": "182b0a2d-d47e-47dd-ced4-e0e29a862197"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0, 0, 0, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text, test_text, Ytrain, Ytest = train_test_split(input_texts, labels)\n",
        "len(train_text)\n",
        "train_text[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "airG-p-smpbQ",
        "outputId": "accb2f56-089d-4ef5-93f9-a83119bb6c53"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['of here and there a tent in grove and orchard',\n",
              " 'the chisel work of an enormous glacier',\n",
              " 'no doubt its grown up some to woods around it',\n",
              " 'had spilled them near the window toward the light',\n",
              " 'when shoeing home across the white']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}
