{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGurZyu+DhrGLHSeGcHTHV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xpdlaldam/nlp/blob/master/HF_audio_course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1."
      ],
      "metadata": {
        "id": "KqZyyLMO2sit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. Background\n",
        "By nature, a sound wave is a continuous signal, meaning it contains an infinite number of signal values in a given time.\n",
        "This poses problems for digital devices which expect finite arrays.\n",
        "To be processed, stored, and transmitted by digital devices, the continuous sound wave needs to be converted into a series of discrete values,\n",
        "known as a digital representation.\n",
        "\n",
        "2. Sampling and sampling rate\n",
        "  2-1. Sampling:\n",
        "    - the process of measuring the value of a continuous signal at fixed time steps. The sampled waveform is discrete, since it contains a finite number of signal values at uniform intervals.\n",
        "\n",
        "  2-2. Sampling rate (also called sampling frequency or frequency) *\n",
        "    - the number of samples taken in \"one second\" and is measured in hertz (Hz)\n",
        "    - how often the samples are taken\n",
        "    - unit: Hz\n",
        "    - High frequency vs low frequency\n",
        "      - High frequency sounds have shorter wavelenghts, often a high pitch sound\n",
        "\n",
        "3. Resampling\n",
        "  - Since sequences are different for audio examples at different sampling rates, it will be challenging for models to generalize between different sampling rates. So \"resampling\" is the process of making the sampling rates match, and is part of preprocessing the audio data.\n",
        "\n",
        "4. Amplitude and bit depth\n",
        "  4-1. Amplitude:\n",
        "    - Sound is made by \"changes in air pressure\" at frequencies that are audible to humans. The amplitude of a sound describes the sound pressure level at any given instant and is measured in decibels (dB). We perceive the amplitude as loudness\n",
        "\n",
        "  4-2. bit depth:\n",
        "    - In digital audio, each audio sample records the amplitude of the audio wave at a point in time. The bit depth of the sample determines how much \"precision\" this amplitude value can be described. The higher the bit depth, the more faithfully the digital representation approximates the original continuous sound wave\n",
        "\n",
        "    - The most common audio bit depths are 16-bit and 24-bit. Each is a binary term, representing the number of possible steps to which the amplitude value can be quantized when itâ€™s converted from continuous to discrete: 65,536 steps for 16-bit audio, a whopping 16,777,216 steps for 24-bit audio. Because quantizing involves rounding off the continuous value to a discrete value, the sampling process introduces noise. The higher the bit depth, the smaller this quantization noise. In practice, the quantization noise of 16-bit audio is already small enough to be inaudible, and using higher bit depths is generally not necessary\n",
        "\n",
        "    - You may also come across 32-bit audio. This stores the samples as floating-point values, whereas 16-bit and 24-bit audio use integer samples. The precision of a 32-bit floating-point value is 24 bits, giving it the same bit depth as 24-bit audio. Floating-point audio samples are expected to lie within the [-1.0, 1.0] range. Since machine learning models naturally work on floating-point data, the audio must first be converted into floating-point format before it can be used to train the model.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pQpCHzkXObqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "array, sampling_rate = librosa.load(librosa.ex(\"trumpet\"))"
      ],
      "metadata": {
        "id": "GL3km8AVYTyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array"
      ],
      "metadata": {
        "id": "8ZWxBS4SpZ45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_rate"
      ],
      "metadata": {
        "id": "tVPBcNwBpduM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Visualize audio"
      ],
      "metadata": {
        "id": "bNIE6mHGXJT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nSa-WV9WhtBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-1. Waveform"
      ],
      "metadata": {
        "id": "D3Fy-NfVXP9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "y-axis: amplitude\n",
        "x-axis: time\n",
        "In other words, each x-axis point corresponds to a single sample value that was taken when this sound was sampled.\n",
        "\"\"\"\n",
        "plt.figure().set_figwidth(12)\n",
        "librosa.display.waveshow(array, sr=sampling_rate)"
      ],
      "metadata": {
        "id": "UPFwCr97OLHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-2. Frequency Spectrum"
      ],
      "metadata": {
        "id": "E8_MCylDXWdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Describes the individual frequencies that make up the signal and how strong they are.\n",
        "\n",
        "Visualizes the amplitudes of the individual frequencies at a fixed point in time.\n",
        "\n",
        "Amplitude (dB) vs Frequency (Hz) graph\n",
        "\n",
        "Tip: While it is possible to plot the spectrum of the entire sound, itâ€™s more useful to look at a small region instead. Here weâ€™ll take the DFT over the first 4096 samples, which is roughly the length of the first note being played\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7S6M4AzBO_Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "dft_input = array[:4096]\n",
        "\n",
        "# calculate the discrete Fourier transform (DFT)\n",
        "window = np.hanning(len(dft_input))\n",
        "windowed_input = dft_input * window\n",
        "dft = np.fft.rfft(windowed_input)\n",
        "\n",
        "# get the amplitude spectrum in decibels\n",
        "amplitude = np.abs(dft)\n",
        "amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)\n",
        "\n",
        "# get the frequency bins\n",
        "frequency = librosa.fft_frequencies(sr=sampling_rate, n_fft=len(dft_input))\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "plt.plot(frequency, amplitude_db)\n",
        "plt.xlabel(\"Frequency (Hz)\")\n",
        "plt.ylabel(\"Amplitude (dB)\")\n",
        "plt.xscale(\"log\")"
      ],
      "metadata": {
        "id": "8Bfzht3GY7vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-3. Spectogram"
      ],
      "metadata": {
        "id": "Jb9EQ4sQp5KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The problem of spectrums is that it only shows a partial snapshot of the frequencies at a given instant => solution: take multiple DFTs each covering only a small slice of time and stack the resulting spectra (plural of specturm) together into a spectrogram\n",
        "\n",
        "A spectogram plots the frequency content of an audio signal as it changes over time. It allows you to see\n",
        "  - time\n",
        "  - frequency\n",
        "  - amplitude\n",
        "all in one graph. The algorithm behind this computation is called STFT or Short Time Fourier Transform\n",
        "\n",
        "Each vertical slice in a spectogram corresponds to a single frequency spectrum. By default, librosa.stft() splits the audio signal into segments of 2048 samples, which gives a good trade-off between frequency resolution and time resolution\n",
        "\n",
        "Since the spectrogram and the waveform are different views of the same data, itâ€™s possible to turn the spectrogram back into the original waveform using the inverse STFT. However, this requires the phase information in addition to the amplitude information. If the spectrogram was generated by a machine learning model, it typically only outputs the amplitudes. In that case, we can use a phase reconstruction algorithm such as the classic Griffin-Lim algorithm, or using a neural network called a vocoder, to reconstruct a waveform from the spectrogram.\n",
        "\n",
        "Spectrograms arenâ€™t just used for visualization. Many machine learning models will take spectrograms as input â€” as opposed to waveforms â€” and produce spectrograms as output.\n",
        "\n",
        "Now that we know what a spectrogram is and how itâ€™s made, letâ€™s take a look at a variant of it widely used for speech processing: the mel spectrogram\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jTIEBayWpx9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "D = librosa.stft(array, n_fft=2048) # default\n",
        "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "librosa.display.specshow(S_db, x_axis=\"time\", y_axis=\"hz\")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "KLlwf5MmsHGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = librosa.stft(array, n_fft=4096) # a higher value returns a more detailed frequency but lower time resolution and slower computation\n",
        "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "librosa.display.specshow(S_db, x_axis=\"time\", y_axis=\"hz\")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "UP8DF_mUycU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-4. Mel Spectogram"
      ],
      "metadata": {
        "id": "TMgtC3ZQgWiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In a standard spectrogram, the frequency axis is linear and is measured in hertz (Hz). However, the human auditory system is more sensitive to changes in lower frequencies than higher frequencies, and this sensitivity decreases logarithmically as frequency increases. The mel scale is a perceptual scale that approximates the non-linear frequency response of the human ear\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Ep7oRBcfaCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "n_mels stands for the number of mel bands to generate. The mel bands define a set of frequency ranges that divide the spectrum into perceptually meaningful components, using a set of filters whose shape and spacing are chosen to mimic the way the human ear responds to different frequencies. Common values for n_mels are 40 or 80. fmax indicates the highest frequency (in Hz) we care about\n",
        "\n",
        "Not all mel spectrograms are the same! There are two different mel scales in common use (\"htk\" and \"slaney\"), and instead of the power spectrogram the amplitude spectrogram may be used. HTK and Slaney are two different formulas for converting between Hz (regular frequency) and mel (perceptual frequency)\n",
        "\n",
        "Caution: Creating a mel spectrogram is a lossy operation as it involves filtering the signal. Converting a mel spectrogram back into a waveform is more difficult than doing this for a regular spectrogram, as it requires estimating the frequencies that were thrown away. This is why machine learning models such as HiFiGAN vocoder are needed to produce a waveform from a mel spectrogram.\n",
        "\n",
        "Compared to a standard spectrogram, a mel spectrogram can capture more meaningful features of the audio signal for human perception, making it a popular choice in tasks such as speech recognition, speaker identification, and music genre classification.\n",
        "\"\"\"\n",
        "\n",
        "S = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=80, fmax=8000)\n",
        "S_dB = librosa.power_to_db(S, ref=np.max) # need power to dB instead of amplitude to dB as the above melspectrogram creates a power spectrogram\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "librosa.display.specshow(S_dB, x_axis=\"time\", y_axis=\"mel\", sr=sampling_rate, fmax=8000)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "aFKWR3cThZ4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Explore Datasets Library"
      ],
      "metadata": {
        "id": "wE_ho2osymzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets[audio]"
      ],
      "metadata": {
        "id": "lxWEJNzDymzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all available language configurations\n",
        "from datasets import get_dataset_config_names\n",
        "configs = get_dataset_config_names(\"PolyAI/minds14\")\n",
        "configs"
      ],
      "metadata": {
        "id": "9wZV63Rnymzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")\n",
        "minds # num_rows: 654 = number of audio files"
      ],
      "metadata": {
        "id": "WdHhS_dpzutE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check original sampling rate: sampling_rate=8000\n",
        "minds.features['audio']"
      ],
      "metadata": {
        "id": "dcmkyOgDE9Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds[0]"
      ],
      "metadata": {
        "id": "NgOe7ApW4uy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Preprocessing an audio dataset"
      ],
      "metadata": {
        "id": "0cFoNIWl3jHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-1. Resampling"
      ],
      "metadata": {
        "id": "K3p0nieX3uRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsampling: change the sampling rate from 8kHz to 16kHz\n",
        "from datasets import Audio\n",
        "minds_resampled = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "minds_resampled.features['audio'] # sampling_rate is now 16_000"
      ],
      "metadata": {
        "id": "SqWuiWdv3p9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-2. Filtering"
      ],
      "metadata": {
        "id": "i4ObH-1XHwrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "You may need to filter the data based on some criteria. One of the common cases involves limiting the audio examples to a certain duration. For instance, we might want to filter out any examples longer than 20s to prevent out-of-memory errors when training a model.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Z6LiWt30Hzpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# access array\n",
        "minds_resampled[0]['audio']['array']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KSpWE7WK6u-",
        "outputId": "f33d88bf-48f9-4a37-aeca-64d4b57ebf9e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.        , 0.00012415, 0.00024414, ..., 0.00071331, 0.0012207 ,\n",
              "       0.00144803], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# use librosa to get example's duration from the audio file\n",
        "new_column = [\n",
        "    librosa.get_duration(y=x[\"array\"], sr=x[\"sampling_rate\"]) for x in minds_resampled[\"audio\"]\n",
        "]\n",
        "new_column"
      ],
      "metadata": {
        "id": "k39_NFLWKs4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_DURATION_IN_SECONDS = 20.0\n",
        "def is_audio_length_in_range(input_length):\n",
        "    return input_length < MAX_DURATION_IN_SECONDS"
      ],
      "metadata": {
        "id": "mVLpQb8DJ1Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds = minds.add_column(\"duration\", new_column)\n",
        "\n",
        "# use ðŸ¤— Datasets' `filter` method to apply the filtering function\n",
        "minds = minds.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
        "\n",
        "# remove the temporary helper column\n",
        "minds = minds.remove_columns([\"duration\"])\n",
        "minds"
      ],
      "metadata": {
        "id": "cvUGJivAJ-IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds_resampled['audio']['array']"
      ],
      "metadata": {
        "id": "EYo7ylYHJaxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications"
      ],
      "metadata": {
        "id": "Ydc7TAHu3VcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Explore Speechbrain's Accent Classification Model using Local .wmf Irish English files"
      ],
      "metadata": {
        "id": "v9ZevIJcD75N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BRANCH = 'develop'\n",
        "!python -m pip install git+https://github.com/speechbrain/speechbrain.git@$BRANCH"
      ],
      "metadata": {
        "id": "cfcrMikXNsRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import speechbrain as sb"
      ],
      "metadata": {
        "id": "JHBLVIUNPxWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from speechbrain.inference import EncoderClassifier\n",
        "classifier = EncoderClassifier.from_hparams(source=\"Jzuluaga/accent-id-commonaccent_ecapa\", savedir=\"pretrained_models/accent-id-commonaccent_ecapa\")\n",
        "\n",
        "# print(text_lab)"
      ],
      "metadata": {
        "id": "IY7b2JfAQJlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import wavfile\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KvTRSyxLTIYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"drive/MyDrive/Colab Notebooks/Data/Audio\")"
      ],
      "metadata": {
        "id": "Zr65PWEyTakE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curr_dir_files = os.listdir()\n",
        "curr_dir_files[0]"
      ],
      "metadata": {
        "id": "Ye0QMseXUKyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the WAV file\n",
        "sample_rate, data = wavfile.read(curr_dir_files[0])\n",
        "\n",
        "print(f\"Sample rate: {sample_rate} Hz\")\n",
        "print(f\"Data shape: {data.shape}\")"
      ],
      "metadata": {
        "id": "eoMdR1pZUNne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "\n",
        "# Play the audio sample\n",
        "Audio(data, rate=sample_rate)"
      ],
      "metadata": {
        "id": "s-An0uPjUxI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Irish Example\n",
        "out_prob, score, index, text_lab = classifier.classify_file(curr_dir_files[0])"
      ],
      "metadata": {
        "id": "Eep5wNQYST-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_lab"
      ],
      "metadata": {
        "id": "gicDMIkXUe7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "id": "svPuOGOdUklQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Challenge"
      ],
      "metadata": {
        "id": "3FxYf8a4dqMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "task1: compare the Irish a sound vs American a and plot\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "w3VhBZiGdtMd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}