{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "v9ZevIJcD75N"
      ],
      "authorship_tag": "ABX9TyPK42leA7b2rGp36FnkKaVj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xpdlaldam/nlp/blob/master/HF_audio_course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1."
      ],
      "metadata": {
        "id": "KqZyyLMO2sit"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1. Background\n",
        "By nature, a sound wave is a continuous signal, meaning it contains an infinite number of signal values in a given time.\n",
        "This poses problems for digital devices which expect finite arrays.\n",
        "To be processed, stored, and transmitted by digital devices, the continuous sound wave needs to be converted into a series of discrete values,\n",
        "known as a digital representation.\n",
        "\n",
        "x. Frequency\n",
        "  - How many times a sound wave oscillates (vibrates) per second. It determines the pitch of a sound. Unit is Hz\n",
        "  - High frequency vs low frequency:\n",
        "    - high frequency sounds have shorter wavelenghts, often a high pitch sound (treble, sharp sound)\n",
        "    - low frequency sounds have longer wavelenghts, often a low pitch sound (bass, flat sound)\n",
        "\n",
        "2. Sampling and sampling rate\n",
        "  2-1. Sampling:\n",
        "    - the process of measuring the value of a continuous signal at fixed time steps. The sampled waveform is discrete, since it contains a finite number of signal values at uniform intervals.\n",
        "\n",
        "  2-2. Sampling rate (also called sampling frequency) *\n",
        "    - the number of samples taken in \"one second\" and is measured in hertz (Hz)\n",
        "    - how often the samples are taken\n",
        "\n",
        "3. Resampling\n",
        "  - Since sequences are different for audio examples at different sampling rates, it will be challenging for models to generalize between different sampling rates. So \"resampling\" is the process of making the sampling rates match, and is part of preprocessing the audio data.\n",
        "\n",
        "4. Amplitude and bit depth\n",
        "  4-1. Amplitude:\n",
        "    - Sound is made by \"changes in air pressure\" at frequencies that are audible to humans. The amplitude of a sound describes the sound pressure level at any given instant and is measured in decibels (dB). We perceive the amplitude as loudness\n",
        "\n",
        "  4-2. bit depth:\n",
        "    - In digital audio, each audio sample records the amplitude of the audio wave at a point in time. The bit depth of the sample determines how much \"precision\" this amplitude value can be described. The higher the bit depth, the more faithfully the digital representation approximates the original continuous sound wave\n",
        "\n",
        "    - The most common audio bit depths are 16-bit and 24-bit. Each is a binary term, representing the number of possible steps to which the amplitude value can be quantized when itâ€™s converted from continuous to discrete: 65,536 steps for 16-bit audio, a whopping 16,777,216 steps for 24-bit audio. Because quantizing involves rounding off the continuous value to a discrete value, the sampling process introduces noise. The higher the bit depth, the smaller this quantization noise. In practice, the quantization noise of 16-bit audio is already small enough to be inaudible, and using higher bit depths is generally not necessary\n",
        "\n",
        "    - You may also come across 32-bit audio. This stores the samples as floating-point values, whereas 16-bit and 24-bit audio use integer samples. The precision of a 32-bit floating-point value is 24 bits, giving it the same bit depth as 24-bit audio. Floating-point audio samples are expected to lie within the [-1.0, 1.0] range. Since machine learning models naturally work on floating-point data, the audio must first be converted into floating-point format before it can be used to train the model.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pQpCHzkXObqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "array, sampling_rate = librosa.load(librosa.ex(\"trumpet\"))"
      ],
      "metadata": {
        "id": "GL3km8AVYTyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array"
      ],
      "metadata": {
        "id": "8ZWxBS4SpZ45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_rate"
      ],
      "metadata": {
        "id": "tVPBcNwBpduM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Visualize audio"
      ],
      "metadata": {
        "id": "bNIE6mHGXJT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import librosa.display\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nSa-WV9WhtBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-1. Waveform"
      ],
      "metadata": {
        "id": "D3Fy-NfVXP9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "y-axis: amplitude\n",
        "x-axis: time\n",
        "In other words, each x-axis point corresponds to a single sample value that was taken when this sound was sampled.\n",
        "\"\"\"\n",
        "plt.figure().set_figwidth(12)\n",
        "librosa.display.waveshow(array, sr=sampling_rate)"
      ],
      "metadata": {
        "id": "UPFwCr97OLHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-2. Frequency Spectrum"
      ],
      "metadata": {
        "id": "E8_MCylDXWdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Describes the individual frequencies that make up the signal and how strong they are.\n",
        "\n",
        "Visualizes the amplitudes of the individual frequencies at a fixed point in time.\n",
        "\n",
        "Amplitude (dB) vs Frequency (Hz) graph\n",
        "\n",
        "Tip: While it is possible to plot the spectrum of the entire sound, itâ€™s more useful to look at a small region instead. Here weâ€™ll take the DFT over the first 4096 samples, which is roughly the length of the first note being played\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "7S6M4AzBO_Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "dft_input = array[:4096]\n",
        "\n",
        "# calculate the discrete Fourier transform (DFT)\n",
        "window = np.hanning(len(dft_input))\n",
        "windowed_input = dft_input * window\n",
        "dft = np.fft.rfft(windowed_input)\n",
        "\n",
        "# get the amplitude spectrum in decibels\n",
        "amplitude = np.abs(dft)\n",
        "amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)\n",
        "\n",
        "# get the frequency bins\n",
        "frequency = librosa.fft_frequencies(sr=sampling_rate, n_fft=len(dft_input))\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "plt.plot(frequency, amplitude_db)\n",
        "plt.xlabel(\"Frequency (Hz)\")\n",
        "plt.ylabel(\"Amplitude (dB)\")\n",
        "plt.xscale(\"log\")"
      ],
      "metadata": {
        "id": "8Bfzht3GY7vz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-3. Spectogram"
      ],
      "metadata": {
        "id": "Jb9EQ4sQp5KE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The problem of spectrums is that it only shows a partial snapshot of the frequencies at a given instant => solution: take multiple DFTs each covering only a small slice of time and stack the resulting spectra (plural of specturm) together into a spectrogram\n",
        "\n",
        "A spectogram plots the frequency content of an audio signal as it changes over time. It allows you to see\n",
        "  - time\n",
        "  - frequency\n",
        "  - amplitude\n",
        "all in one graph. The algorithm behind this computation is called STFT or Short Time Fourier Transform\n",
        "\n",
        "Each vertical slice in a spectogram corresponds to a single frequency spectrum. By default, librosa.stft() splits the audio signal into segments of 2048 samples, which gives a good trade-off between frequency resolution and time resolution\n",
        "\n",
        "Since the spectrogram and the waveform are different views of the same data, itâ€™s possible to turn the spectrogram back into the original waveform using the inverse STFT. However, this requires the phase information in addition to the amplitude information. If the spectrogram was generated by a machine learning model, it typically only outputs the amplitudes. In that case, we can use a phase reconstruction algorithm such as the classic Griffin-Lim algorithm, or using a neural network called a vocoder, to reconstruct a waveform from the spectrogram.\n",
        "\n",
        "Spectrograms arenâ€™t just used for visualization. Many machine learning models will take spectrograms as input â€” as opposed to waveforms â€” and produce spectrograms as output.\n",
        "\n",
        "Now that we know what a spectrogram is and how itâ€™s made, letâ€™s take a look at a variant of it widely used for speech processing: the mel spectrogram\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "jTIEBayWpx9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "D = librosa.stft(array, n_fft=2048) # default\n",
        "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "librosa.display.specshow(S_db, x_axis=\"time\", y_axis=\"hz\")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "KLlwf5MmsHGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = librosa.stft(array, n_fft=4096) # a higher value returns a more detailed frequency but lower time resolution and slower computation\n",
        "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "librosa.display.specshow(S_db, x_axis=\"time\", y_axis=\"hz\")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "UP8DF_mUycU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2-4. Mel Spectogram"
      ],
      "metadata": {
        "id": "TMgtC3ZQgWiy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In a standard spectrogram, the frequency axis is linear and is measured in hertz (Hz). However, the human auditory system is more sensitive to changes in lower frequencies than higher frequencies, and this sensitivity decreases logarithmically as frequency increases. The mel scale is a perceptual scale that approximates the non-linear frequency response of the human ear\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "2Ep7oRBcfaCj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "n_mels stands for the number of mel bands to generate. The mel bands define a set of frequency ranges that divide the spectrum into perceptually meaningful components, using a set of filters whose shape and spacing are chosen to mimic the way the human ear responds to different frequencies. Common values for n_mels are 40 or 80. fmax indicates the highest frequency (in Hz) we care about\n",
        "\n",
        "Not all mel spectrograms are the same! There are two different mel scales in common use (\"htk\" and \"slaney\"), and instead of the power spectrogram the amplitude spectrogram may be used. HTK and Slaney are two different formulas for converting between Hz (regular frequency) and mel (perceptual frequency)\n",
        "\n",
        "Caution: Creating a mel spectrogram is a lossy operation as it involves filtering the signal. Converting a mel spectrogram back into a waveform is more difficult than doing this for a regular spectrogram, as it requires estimating the frequencies that were thrown away. This is why machine learning models such as HiFiGAN vocoder are needed to produce a waveform from a mel spectrogram.\n",
        "\n",
        "Compared to a standard spectrogram, a mel spectrogram can capture more meaningful features of the audio signal for human perception, making it a popular choice in tasks such as speech recognition, speaker identification, and music genre classification.\n",
        "\"\"\"\n",
        "\n",
        "S = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=80, fmax=8000)\n",
        "S_dB = librosa.power_to_db(S, ref=np.max) # need power to dB instead of amplitude to dB as the above melspectrogram creates a power spectrogram\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "librosa.display.specshow(S_dB, x_axis=\"time\", y_axis=\"mel\", sr=sampling_rate, fmax=8000)\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "aFKWR3cThZ4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Explore Datasets Library"
      ],
      "metadata": {
        "id": "wE_ho2osymzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soundfile librosa datasets[audio] torchcodec"
      ],
      "metadata": {
        "id": "oi4ytBxaXORq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# restart session"
      ],
      "metadata": {
        "id": "CxCBic9KYgGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all available language configurations\n",
        "from datasets import get_dataset_config_names\n",
        "configs = get_dataset_config_names(\"PolyAI/minds14\")\n",
        "configs"
      ],
      "metadata": {
        "id": "9wZV63Rnymzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")\n",
        "minds # num_rows: 654 = number of audio files"
      ],
      "metadata": {
        "id": "WdHhS_dpzutE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds.column_names"
      ],
      "metadata": {
        "id": "p6hNCyq2X-FF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check original sampling rate: sampling_rate=8000\n",
        "minds.features['audio']"
      ],
      "metadata": {
        "id": "dcmkyOgDE9Cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds[0]"
      ],
      "metadata": {
        "id": "NgOe7ApW4uy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Preprocessing an audio dataset"
      ],
      "metadata": {
        "id": "0cFoNIWl3jHc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-1. Resampling"
      ],
      "metadata": {
        "id": "K3p0nieX3uRw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upsampling: change the sampling rate from 8kHz to 16kHz\n",
        "from datasets import Audio\n",
        "minds_resampled = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
        "minds_resampled.features['audio'] # sampling_rate is now 16_000"
      ],
      "metadata": {
        "id": "SqWuiWdv3p9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-2. Filtering"
      ],
      "metadata": {
        "id": "i4ObH-1XHwrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "You may need to filter the data based on some criteria. One of the common cases involves limiting the audio examples to a certain duration. For instance, we might want to filter out any examples longer than 20s to prevent out-of-memory errors when training a model.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Z6LiWt30Hzpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# access array\n",
        "minds_resampled[0]['audio']['array']"
      ],
      "metadata": {
        "id": "9KSpWE7WK6u-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(minds_resampled[0]['audio']['array'])"
      ],
      "metadata": {
        "id": "sWle4gwgihVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(minds_resampled[1]['audio']['array'])"
      ],
      "metadata": {
        "id": "W7hXPndRipQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(minds_resampled[0]['audio']['array']) / minds_resampled[0]['audio']['sampling_rate'] # duration in seconds for the first audio file\n",
        "# ex) 16_000 samples: 1 second = 124_830 samples : x second => x = 7.8 seconds"
      ],
      "metadata": {
        "id": "AV9uoet_i1lF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds_resampled[0]['audio']['sampling_rate']"
      ],
      "metadata": {
        "id": "AUH1F43mZ010"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa"
      ],
      "metadata": {
        "id": "qdxvVeQIaNQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(minds_resampled)"
      ],
      "metadata": {
        "id": "A1MKtFC4bbuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use librosa to get the audio's duration from the audio file\n",
        "new_column = [\n",
        "    librosa.get_duration(y=x[\"array\"], sr=x[\"sampling_rate\"]) for x in minds_resampled[\"audio\"]\n",
        "]\n",
        "new_column"
      ],
      "metadata": {
        "id": "k39_NFLWKs4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_column[0] # duration in seconds of the first audio file"
      ],
      "metadata": {
        "id": "Qcev4JzSl5ji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_column)"
      ],
      "metadata": {
        "id": "YgskZp7PkdEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_DURATION_IN_SECONDS = 20.0\n",
        "def is_audio_length_in_range(input_length):\n",
        "    return input_length < MAX_DURATION_IN_SECONDS"
      ],
      "metadata": {
        "id": "mVLpQb8DJ1Bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds_resampled = minds_resampled.add_column(\"duration\", new_column)\n",
        "\n",
        "# use ðŸ¤— Datasets' `filter` method to apply the filtering function\n",
        "minds_resampled = minds_resampled.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
        "\n",
        "# remove the temporary helper column\n",
        "minds_resampled = minds_resampled.remove_columns([\"duration\"])\n",
        "minds_resampled # reduced from 654 to 624 audio files (removed files exceeding 20 seconds)"
      ],
      "metadata": {
        "id": "cvUGJivAJ-IB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds_resampled.features['audio'] # check upsampled sampling_rate"
      ],
      "metadata": {
        "id": "4wFUqVUL9zCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4-3. Adjust to Model Input using pre-trained Whisper model"
      ],
      "metadata": {
        "id": "AftDB819xsqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa"
      ],
      "metadata": {
        "id": "zfa7aUpd2Sek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "For every supported audio model, ðŸ¤— Transformers offer a \"feature extractor\" class that can convert raw audio data into the input features the model expects\n",
        "\n",
        "How the Whisper feature extractor works:\n",
        "  First, the Whisper feature extractor pads/truncates a batch of audio examples such that all examples have an input length of 30s. Since all elements in the batch are padded/truncated to a maximum length in the input space, there is no need for an attention mask. Hence Whisper is trained to operate without an attention mask and infer directly from the speech signals where to ignore the inputs\n",
        "\n",
        "  The second operation that the Whisper feature extractor performs is converting the padded audio arrays to log-mel spectrograms\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "BTwocQ7_yblp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
      ],
      "metadata": {
        "id": "Tq9Zux9A1Uyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(example):\n",
        "    audio = example[\"audio\"]\n",
        "\n",
        "    if audio[\"sampling_rate\"] != 16000:\n",
        "        audio_array = librosa.resample(\n",
        "            audio[\"array\"], orig_sr=audio[\"sampling_rate\"], target_sr=16000\n",
        "        )\n",
        "        audio = {\"array\": audio_array, \"sampling_rate\": 16000}\n",
        "\n",
        "    features = feature_extractor(\n",
        "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], padding=True\n",
        "    )\n",
        "    return features"
      ],
      "metadata": {
        "id": "SuTnAXYY1-9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds_resampled_whisper = minds_resampled.map(prepare_dataset)\n",
        "minds_resampled_whisper"
      ],
      "metadata": {
        "id": "igai7vEd2DLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## plot mel spectogram of first audio file\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "example = minds_resampled_whisper[0]\n",
        "input_features = example[\"input_features\"]\n",
        "\n",
        "plt.figure().set_figwidth(12)\n",
        "librosa.display.specshow(\n",
        "    np.asarray(input_features[0]),\n",
        "    x_axis=\"time\",\n",
        "    y_axis=\"mel\",\n",
        "    sr=feature_extractor.sampling_rate,\n",
        "    hop_length=feature_extractor.hop_length,\n",
        ")\n",
        "plt.colorbar()"
      ],
      "metadata": {
        "id": "ZOy9bk_H-IfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Interpret the above audio frequency distribution:\n",
        "  - Low Frequencies (0-512 Hz):\n",
        "  Strong energy (red/orange) in the lower frequencies\n",
        "  This is typical for human speech - contains the fundamental frequency of the voice\n",
        "  Indicates a male voice or low-pitched female voice (most energy concentrated here)\n",
        "\n",
        "  - Mid Frequencies (512-2048 Hz):\n",
        "  Moderate energy (light blue/orange)\n",
        "  Contains vowel formants and voice harmonics\n",
        "  Critical for speech intelligibility\n",
        "\n",
        "  - High Frequencies (2048-4096 Hz):\n",
        "  Less energy (mostly blue)\n",
        "  Contains consonants and fricatives (s, sh, f sounds)\n",
        "  Some vertical stripes suggest consonant sounds\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "znGrNV5OIeYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds_resampled_whisper[0]['audio']"
      ],
      "metadata": {
        "id": "80a-dQAD_871"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "\n",
        "audio = minds_resampled_whisper[0]['audio']\n",
        "Audio(audio['array'], rate=audio['sampling_rate'])"
      ],
      "metadata": {
        "id": "_k7Sunsk_mlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Streaming Audio Data"
      ],
      "metadata": {
        "id": "G9kai7APHitP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ðŸ¤— Datasets comes to the rescue by offering the streaming mode. Streaming allows us to load the data progressively as we iterate over the dataset. Rather than downloading the whole dataset at once, we load the dataset one example at a time. We iterate over the dataset, loading and preparing examples on the fly when they are needed. This way, we only ever load the examples that weâ€™re using, and not the ones that weâ€™re not! Once weâ€™re done with an example sample, we continue iterating over the dataset and load the next one\n",
        "\n",
        "There is one caveat to streaming mode. When downloading a full dataset without streaming, both the raw data and processed data are saved locally to disk. If we want to re-use this dataset, we can directly load the processed data from disk, skipping the download and processing steps. Consequently, we only have to perform the downloading and processing operations once, after which we can re-use the prepared data.\n",
        "\n",
        "With streaming mode, the data is not downloaded to disk. Thus, neither the downloaded nor pre-processed data are cached. If we want to re-use the dataset, the streaming steps must be repeated, with the audio files loaded and processed on the fly again. For this reason, it is advised to download datasets that you are likely to use multiple times.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "TkxPGrb6Nf0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Audio Classification using pipeline"
      ],
      "metadata": {
        "id": "Oy3tn8FJ8y1Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Uninstall everything\n",
        "!pip uninstall -y datasets transformers torchcodec torchaudio\n",
        "\n",
        "# Install working combination\n",
        "!pip install datasets==2.18.0 transformers soundfile librosa\n",
        "\n",
        "print(\"âœ“ Installation complete\")\n",
        "print(\"âš ï¸  RESTART RUNTIME NOW\")"
      ],
      "metadata": {
        "id": "ObwNJh8OAhD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Audio\n",
        "\n",
        "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")\n",
        "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000)) # upscaled sampling_rate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_1Lb_lw87mS",
        "outputId": "17551567-b4cd-4a5a-cfc3-bb823cb0069e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "minds[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UW_LpGngBGZQ",
        "outputId": "4b660056-c03b-405a-8cf4-43eb93b7a907"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'path': 'en-AU~PAY_BILL/response_4.wav',\n",
              " 'audio': {'path': 'response_4.wav',\n",
              "  'array': array([2.36119668e-05, 1.92324660e-04, 2.19284790e-04, ...,\n",
              "         9.40907281e-04, 1.16613181e-03, 7.20883254e-04]),\n",
              "  'sampling_rate': 16000},\n",
              " 'transcription': 'I would like to pay my electricity bill using my card can you please assist',\n",
              " 'english_transcription': 'I would like to pay my electricity bill using my card can you please assist',\n",
              " 'intent_class': 13,\n",
              " 'lang_id': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\n",
        "    \"audio-classification\",\n",
        "    model=\"anton-l/xtreme_s_xlsr_300m_minds14\",\n",
        ")"
      ],
      "metadata": {
        "id": "coaU0psL-GmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier(minds[0][\"audio\"][\"array\"])"
      ],
      "metadata": {
        "id": "IJlaGw3C-X04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WJlctLY4_p7k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds.features"
      ],
      "metadata": {
        "id": "8G_veE-5_TMF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "minds[0]"
      ],
      "metadata": {
        "id": "ewMR64CV_Q6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = minds[0][\"audio\"]\n",
        "\n",
        "# Pass as dictionary with both array and sampling_rate\n",
        "result = classifier({\n",
        "    \"array\": sample[\"array\"],\n",
        "    \"sampling_rate\": sample[\"sampling_rate\"]\n",
        "})"
      ],
      "metadata": {
        "id": "y03zchxo_H8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Applications"
      ],
      "metadata": {
        "id": "Ydc7TAHu3VcI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Explore Speechbrain's Accent Classification Model using Local .wmf Irish English files"
      ],
      "metadata": {
        "id": "v9ZevIJcD75N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BRANCH = 'develop'\n",
        "!python -m pip install git+https://github.com/speechbrain/speechbrain.git@$BRANCH"
      ],
      "metadata": {
        "id": "cfcrMikXNsRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import speechbrain as sb"
      ],
      "metadata": {
        "id": "JHBLVIUNPxWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "from speechbrain.inference import EncoderClassifier\n",
        "classifier = EncoderClassifier.from_hparams(source=\"Jzuluaga/accent-id-commonaccent_ecapa\", savedir=\"pretrained_models/accent-id-commonaccent_ecapa\")\n",
        "\n",
        "# print(text_lab)"
      ],
      "metadata": {
        "id": "IY7b2JfAQJlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import wavfile\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "KvTRSyxLTIYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(\"drive/MyDrive/Colab Notebooks/Data/Audio\")"
      ],
      "metadata": {
        "id": "Zr65PWEyTakE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "curr_dir_files = os.listdir()\n",
        "curr_dir_files[0]"
      ],
      "metadata": {
        "id": "Ye0QMseXUKyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the WAV file\n",
        "sample_rate, data = wavfile.read(curr_dir_files[0])\n",
        "\n",
        "print(f\"Sample rate: {sample_rate} Hz\")\n",
        "print(f\"Data shape: {data.shape}\")"
      ],
      "metadata": {
        "id": "eoMdR1pZUNne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio\n",
        "import numpy as np\n",
        "\n",
        "# Play the audio sample\n",
        "Audio(data, rate=sample_rate)"
      ],
      "metadata": {
        "id": "s-An0uPjUxI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Irish Example\n",
        "out_prob, score, index, text_lab = classifier.classify_file(curr_dir_files[0])"
      ],
      "metadata": {
        "id": "Eep5wNQYST-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_lab"
      ],
      "metadata": {
        "id": "gicDMIkXUe7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index"
      ],
      "metadata": {
        "id": "svPuOGOdUklQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Challenge"
      ],
      "metadata": {
        "id": "3FxYf8a4dqMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "task1: compare the Irish a sound vs American a and plot\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "w3VhBZiGdtMd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}